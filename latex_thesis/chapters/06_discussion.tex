\chapter{Discussion}
\label{ch:discussion}

\section{Inference as a Database Operation}

While \GSIP{} is currently implemented as a standalone Python pipeline, its architectural principles align closely with the future of \textbf{Array Databases} like \texttt{Rasdaman}.

\subsection{The "Move Code to Data" Paradigm}
Traditional \EO{} workflows follow an ETL (Extract-Transform-Load) pattern:
\begin{enumerate}
    \item \textbf{Extract:} Download GeoTIFFs from S3/Database to local disk.
    \item \textbf{Transform:} Run Python script (\GSIP{}).
    \item \textbf{Load:} Upload results back.
\end{enumerate}

For petabyte-scale archives, the "Extract" step is the bottleneck. The network bandwidth cannot keep up with the compute speed.

The architecture of \GSIP{}---processing independent, stateless chunks with a defined halo context---makes it an ideal candidate for a \textbf{User Defined Function (UDF)}. One can envision a \texttt{Rasdaman} \texttt{WCPS} (Web Coverage Processing Service) query:

\begin{lstlisting}[language=SQL, caption=Hypothetical WCPS Query invoking GSIP]
SELECT flood_detection(
    s2_image,
    model_uri='prithvi-100m.pt'
)
FROM Sentinel2_Collection
WHERE ...
\end{lstlisting}

In this vision, the database kernel itself handles the "Physical Tiling" and distribution to worker nodes. \GSIP{} becomes the "Kernel Function" executing the neural network on the local data shard. This effectively eliminates the network transfer entirely.

\subsection{Comparison with Existing Database Approaches}
Existing approaches (e.g., \texttt{SciDB}, \texttt{TileDB}) offer UDFs, but they are often restricted to C++ or simple arithmetic operations. Integrating a full Python Deep Learning stack (PyTorch + Transformers) into a database kernel is non-trivial due to dependency management. \GSIP{}'s container-ready, modular design is a step towards this integration, potentially running as a sidecar service invoked by the database.

\section{Foundation Models in Production}

The shift towards Foundation Models (e.g., Prithvi, SatMAE) represents a "Fine-Tuning Paradigm." Unlike training small \CNNs{} from scratch, researchers now adapt massive pre-trained backbones.

\subsection{The Computational Cost}
A ResNet-50 has ~25 million parameters. Prithvi-100M has 100 million. Future models will likely exceed 1 billion.

Running these models requires significant memory. \GSIP{}'s \textbf{Zone of Responsibility (ZoR)} approach is forward-looking. By strictly managing the memory budget, it allows these heavy models to run on "Edge" hardware (e.g., a field laptop) by simply processing smaller chunks. A naive script would immediately crash. \GSIP{} trades time for memory, allowing the computation to succeed eventually.

\section{Limitations}

Despite its robustness, the current system faces limitations:
\begin{itemize}
    \item \textbf{Storage Explosion:} The output of segmentation models---particularly the raw probability maps (Float32)---is $4\times$ larger than the input imagery (Uint16). Storing full probability distributions for every pixel is storage-prohibitive. Future work must investigate \textbf{Logit Quantization} (converting Float32 to Int8) or on-the-fly thresholding.
    \item \textbf{Temporal Context:} Currently, \GSIP{} operates on 2D spatial slices ($X, Y$). Many phenomena (crop growth, flood recession) are best understood in 3D ($X, Y, T$). Extending the tiling algebra to 3D "Time-Cubes" is theoretically possible but adds exponential complexity to the boundary reconstruction logic.
\end{itemize}