diff --git a/build/lib.linux-x86_64-cpython-310/cython_writer.cpython-310-x86_64-linux-gnu.so b/build/lib.linux-x86_64-cpython-310/cython_writer.cpython-310-x86_64-linux-gnu.so
deleted file mode 100755
index 5f7fdfd..0000000
Binary files a/build/lib.linux-x86_64-cpython-310/cython_writer.cpython-310-x86_64-linux-gnu.so and /dev/null differ
diff --git a/build/temp.linux-x86_64-cpython-310/cython_writer.o b/build/temp.linux-x86_64-cpython-310/cython_writer.o
deleted file mode 100644
index 16d5f33..0000000
Binary files a/build/temp.linux-x86_64-cpython-310/cython_writer.o and /dev/null differ
diff --git a/doc.md b/doc.md
deleted file mode 100644
index 8fb18ca..0000000
--- a/doc.md
+++ /dev/null
@@ -1,215 +0,0 @@
-Scalable Sentinel-2 Segmentation Pipeline
-
-This documentation details the architecture, methodology, and implementation of a high-throughput, scalable deep learning pipeline designed for semantic segmentation of large-scale Sentinel-2 (BigEarthNet) imagery. The system is engineered to handle gigapixel-scale images by managing memory and maximizing GPU utilization through a novel chunking and weighted mosaicking approach.
-
-I. Architectural Overview
-
-The core challenge in processing large remote sensing tiles ($10,980 \times 10,980$ pixels) is the memory constraint on both system RAM and GPU VRAM. This pipeline addresses this by adopting a Producer-Consumer model based on three key stages:
-
-Chunking: Splitting the large image into manageable, sequential chunks that fit in RAM.
-
-Overlapping Patching & Inference (Producer): Subdividing the current chunk into highly-overlapping patches for GPU inference. This is the GPU-bound stage.
-
-Weighted Mosaicking & Writing (Consumer): Blending the overlapping patch predictions and writing the final results to disk in real-time. This is the CPU/I/O-bound stage, running in parallel with the producer.
-
-II. Core Methodology
-
-1. Chunking and Memory Management
-
-To avoid loading the entire $10,980 \times 10,980$ image (which could exceed 5GB for 10 bands of $\text{uint16}$ data) into system memory, the image is processed in large, square Chunks.
-
-PREFERRED_CHUNK_SIZE ($\text{e.g., } 5000$): Defined in config.py, this sets the approximate pixel dimension of the chunk.
-
-Adaptive Sizing: The system calculates the final $\text{CHUNK\_SIZE}$ to ensure it is a multiple of all Sentinel-2 band resolutions (e.g., 10m, 20m, 60m) and the $\text{PATCH\_SIZE}$. This prevents resampling artifacts at chunk boundaries.
-
-Sequential $\text{I/O}$: Only one chunk's worth of data is loaded from the multi-band $\text{GeoTIFF}$ files into $\text{NumPy}$ arrays at a time using $\text{Rasterio}$'s Window reading capabilities.
-
-2. Overlapping Patch Generation
-
-The neural network is trained on small patches (e.g., $120 \times 120$ pixels). Simple non-overlapping patching would create severe checkerboard artifacts due to prediction inconsistencies at patch boundaries.
-
-Overlap Stride: Patches are extracted from the chunk using a stride of $\text{PATCH\_SIZE} // 2$ (half the patch size).
-
-$50\%$ Overlap: This stride ensures that every pixel in the interior of the chunk is covered by four different patches. Pixels near the chunk edges are covered by two or one patch.
-
-Purpose: This extensive overlap is the prerequisite for the Weighted Mosaicking process, allowing for smooth, artifact-free transitions between predicted patches.
-
-3. Probability Calculation and Fusion (Weighted Mosaicking)
-
-This is the most critical step for generating visually and numerically high-quality segmentation maps.
-
-The Problem: Simply averaging the probabilities from overlapping patches still yields a blurred result with visible seams.
-
-The Solution: Use a spatial weighting function to emphasize the prediction reliability near the center of the patch and de-emphasize the less reliable predictions at the patch edges.
-
-Sinusoidal Weighting Mask: The system generates a 2D sinusoidal window function (np.sin(x)) that is:
-
-Maximum (1.0) at the center of the patch.
-
-Minimum (0.0) at the edges of the patch.
-
-Accumulation:
-
-$$P_{\text{blended}}(x, y) = \frac{\sum_{i \in \text{patches}(x,y)} P_{i}(x, y) \cdot W_{i}(x, y)}{\sum_{i \in \text{patches}(x,y)} W_{i}(x, y)}$$
-
-Where:
-
-$P_i(x, y)$ is the probability from patch $i$ at pixel location $(x, y)$.
-
-$W_i(x, y)$ is the weight (sinusoidal mask value) from patch $i$ at pixel $(x, y)$.
-
-This weighted averaging technique effectively smooths the transitions, making the final classification map appear continuous and homogeneous.
-
-4. Uncertainty Quantification
-
-Beyond the final classification mask, the pipeline generates several products for quantifying prediction confidence:
-
-Maximum Probability ($\text{MaxProb}$): The probability of the final dominant class.
-
-Entropy: A measure of the statistical uncertainty of the class prediction for a given pixel, calculated as:
-
-$$E = - \sum_{c=1}^{N} P_c \log_2(P_c)$$
-
-High entropy indicates the model is highly uncertain (probabilities are spread out).
-
-Gap: The difference between the highest and second-highest probability ($\text{max} - \text{second\_max}$). A smaller gap suggests the pixel is close to a decision boundary.
-
-III. Component Deep Dive
-
-config.py
-
-Parameter
-
-Type
-
-Details
-
-PATCH_SIZE
-
-$\text{int}$
-
-Standardized input size for the deep learning model ($\text{e.g., } 120$).
-
-BANDS
-
-$\text{int}$
-
-The number of Sentinel-2 bands to use ($\text{10}$ or $\text{12}$).
-
-PREFERRED_CHUNK_SIZE
-
-$\text{int}$
-
-Determines the chunk dimensions before adaptive adjustment.
-
-NORM_M, NORM_S
-
-$\text{torch.Tensor}$
-
-Pre-calculated $\text{mean}$ and $\text{standard deviation}$ for the $\text{10}$ bands, used for $\text{Z-score}$ normalization on the $\text{GPU}$.
-
-GPU_BATCH_SIZE
-
-$\text{int}$
-
-Number of patches processed in one $\text{GPU}$ forward pass.
-
-utils.py
-
-Provides necessary setup, data normalization, and the core $\text{GPU}$ execution logic.
-
-Dependency Fallback: Contains robust $\text{try/except}$ logic to define placeholder classes and constants (like STANDARD_BANDS, NEW_LABELS, and a dummy BigEarthNetv2_0_ImageClassifier) if external libraries are not available, ensuring the script is runnable even in limited environments.
-
-run_gpu_inference: This is the function that is threaded into the producer-consumer model. It converts $\text{NumPy}$ data to $\text{PyTorch}$ $\text{tensors}$, applies normalization using $\text{config}$ values, runs the $\text{model.forward}$ pass, and applies torch.sigmoid to get probabilities.
-
-save_color_mask_preview: Utility to convert the final $\text{GeoTIFF}$ class indices into a visually appealing $\text{RGB}$ $\text{PNG}$ preview using LABEL_COLOR_MAP.
-
-data_loader.py
-
-Focuses on robust $\text{I/O}$ for large $\text{GeoTIFF}$ files.
-
-_read_all_bands_for_chunk: Handles the complexity of reading a single rectangular window (Window) that spans multiple band files with different spatial resolutions (10m, 20m, 60m).
-
-It reads the highest-resolution band (B02) first to establish the target dimensions.
-
-Lower-resolution bands are read and then resampled using bilinear interpolation (Resampling.bilinear) to match the $\text{10m}$ resolution, ensuring all bands align perfectly in the final chunk array.
-
-segmentator.py (Orchestrator)
-
-The main logic file. It defines the $\text{Producer-Consumer}$ threads and coordinates the entire workflow.
-
-main(tile_folder, output_directory):
-
-Initializes all output $\text{GeoTIFF}$ files using $\text{Rasterio}$ with appropriate profiles (tiled=True, compress='lzw').
-
-Sets up the result_queue to pass $\text{GPU}$ results from the producer to the consumer.
-
-Starts the mosaicking_worker thread.
-
-Producer Loop (in main):
-
-Iterates through all calculated $\text{Chunk}$ coordinates.
-
-Calls $\text{I/O}$ ($\text{load\_bands\_in\_window}$) $\rightarrow$ $\text{Patching}$ ($\text{cut\_into\_patches}$) $\rightarrow$ $\text{Inference}$ ($\text{run\_gpu\_inference}$).
-
-Pushes the results (probabilities, coordinates) to the result_queue.
-
-Consumer Thread (mosaicking_worker):
-
-Runs indefinitely, waiting for results in the queue.
-
-Pulls results and calls the CPU-intensive accumulate_probs function (mosaicking).
-
-Calculates $\text{MaxProb}$, $\text{Entropy}$, and $\text{Gap}$ from the blended probabilities.
-
-Writes the final $\text{NumPy}$ chunk data to the opened $\text{GeoTIFF}$ files using $\text{Rasterio}$'s write method with the correct window coordinates.
-
-main.py
-
-This serves as the simple execution wrapper. It defines the input and output directories and calls segmentator.main, providing a clean entry point for the pipeline.
-
-IV. Output Products
-
-The pipeline produces multiple $\text{GeoTIFF}$ files for downstream GIS analysis, all sharing the original $\text{GeoTIFF}$ coordinate reference system (CRS) and geospatial transform.
-
-Output File ($\text{GeoTIFF}$)
-
-Data Type
-
-Description
-
-*_class.tif
-
-$\text{uint8}$
-
-The final, highest-confidence land cover classification mask (index of the class label).
-
-*_maxprob.tif
-
-$\text{float32}$
-
-The confidence level (probability) of the dominant class assigned in *_class.tif.
-
-*_entropy.tif
-
-$\text{float32}$
-
-The uncertainty measure, useful for highlighting areas where the model was confused.
-
-*_gap.tif
-
-$\text{float32}$
-
-The margin of confidence between the top two classes.
-
-*_probs.tif
-
-$\text{float32}$
-
-A multi-band $\text{GeoTIFF}$ containing the full probability vector for every class at every pixel (optional).
-
-preview.png
-
-$\text{PNG}$
-
-A low-resolution color visualization of the classification map.
\ No newline at end of file
diff --git a/pyproject.toml b/pyproject.toml
index a5b3529..d5b45d9 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -13,6 +13,10 @@ classifiers = [
     "License :: OSI Approved :: MIT License",
     "Operating System :: OS Independent",
 ]
+dependencies = [
+    "pandas",
+    "psutil",
+]
 
 [project.urls]
 "Homepage" = "https://github.com/your-username/BigEarthNetv2.0"
diff --git a/src/ben_v2/data.py b/src/ben_v2/data.py
index 6c2ebb8..1efde85 100644
--- a/src/ben_v2/data.py
+++ b/src/ben_v2/data.py
@@ -29,8 +29,8 @@ def _find_band_path(tile_folder: Path, band_name: str) -> Path | None:
     """Finds the path for a given band in the tile folder (supports .jp2 and .tif)."""
     # NOTE: This function is safe because it only runs once in the main process during init.
     for ext in ['.jp2','.tif']:
-        # Assuming filename structure is like S2A_MSIL1C_..._B02.jp2 or similar
-        candidate = next(tile_folder.glob(f"*{band_name}*{ext}"), None)
+        # Use rglob for recursive search
+        candidate = next(tile_folder.rglob(f"*{band_name}*{ext}"), None)
         if candidate:
             return candidate
     return None
diff --git a/src/ben_v2/main.py b/src/ben_v2/main.py
index 3ce33ee..038f5c9 100644
--- a/src/ben_v2/main.py
+++ b/src/ben_v2/main.py
@@ -12,7 +12,7 @@ from . import config
 from .process import main as segmentator_main
 from . import extra_generators
 
-def main(TILE_FOLDER: str, OUTPUT_FOLDER: str):
+def main(TILE_FOLDER: str, OUTPUT_FOLDER: str, model=None):
     """
     Orchestrates the scalable analysis of a single input scene.
     """
@@ -41,10 +41,3 @@ def main(TILE_FOLDER: str, OUTPUT_FOLDER: str):
     print("\n--- Pipeline Complete ---")
     print(f"Total Wall Time: {total_time:.2f} seconds")
 
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(description='Run BigEarthNetv2.0 segmentation pipeline.')
-    parser.add_argument('--tile_folder', type=str, required=True, help='Path to the Sentinel-2 tile folder.')
-    parser.add_argument('--output_folder', type=str, required=True, help='Path to the output folder.')
-    args = parser.parse_args()
-
-    main(args.tile_folder, args.output_folder)
diff --git a/src/ben_v2/process.py b/src/ben_v2/process.py
index 71e0485..327eff3 100644
--- a/src/ben_v2/process.py
+++ b/src/ben_v2/process.py
@@ -31,19 +31,15 @@ PROB_COMPRESS = "lzw"
 # ------------------------------------------------------------
 def _find_band_path(tile_folder: Path, band_name: str) -> Path | None:
     for ext in ['.jp2','.tif']:
-        candidate = next(tile_folder.glob(f"*{band_name}*{ext}"), None)
+        candidate = next(tile_folder.rglob(f"*{band_name}*{ext}"), None)
         if candidate:
             return candidate
     return None
 
 # ------------------------------------------------------------
-def accumulate_probs(results: np.ndarray, coords: List[Tuple[int, int]], H: int, W: int, patch_size: int, n_classes: int) -> np.ndarray:
+def accumulate_probs(results: np.ndarray, coords: List[Tuple[int, int]], H: int, W: int, patch_size: int, n_classes: int, weight_mask: np.ndarray, weight_mask_count: np.ndarray) -> np.ndarray:
     avg = np.zeros((H, W, n_classes), dtype=np.float32)
     count = np.zeros((H, W, 1), dtype=np.float32)
-    window_2d = np.outer(np.sin(np.linspace(0, np.pi, patch_size))**2,
-                         np.sin(np.linspace(0, np.pi, patch_size))**2).astype(np.float32)
-    weight_mask = np.tile(window_2d[:,:,np.newaxis], (1, 1, n_classes))
-    weight_mask_count = np.tile(window_2d[:,:,np.newaxis], (1, 1, 1))
     for i, (r0, c0) in enumerate(coords):
         prob = results[i]
         weighted_prob = prob[np.newaxis, np.newaxis, :] * weight_mask
@@ -54,7 +50,7 @@ def accumulate_probs(results: np.ndarray, coords: List[Tuple[int, int]], H: int,
     return avg
 
 # ------------------------------------------------------------
-def main(tile_folder: str, crop_limit=None, output_directory: str | None = None, extra_data_generators: List[Callable] | None = None):
+def main(tile_folder: str, crop_limit=None, output_directory: str | None = None, extra_data_generators: List[Callable] | None = None, model=None):
     t0 = time.time()
     tile = Path(tile_folder)
     # Ensure output_directory is used correctly
@@ -142,7 +138,15 @@ def main(tile_folder: str, crop_limit=None, output_directory: str | None = None,
     with open(out_path / f"{tile.name}_classmap.json", "w") as f:
         json.dump(class_map_data, f, indent=2)
 
-    model = BigEarthNetv2_0_ImageClassifier.from_pretrained(REPO_ID).to(DEVICE).eval()
+    if model is None:
+        model = BigEarthNetv2_0_ImageClassifier.from_pretrained(REPO_ID).to(DEVICE).eval()
+
+    # --- Pre-calculate weighting masks ---
+    n_classes = len(NEW_LABELS)
+    window_2d = np.outer(np.sin(np.linspace(0, np.pi, PATCH_SIZE))**2,
+                         np.sin(np.linspace(0, np.pi, PATCH_SIZE))**2).astype(np.float32)
+    weight_mask = np.tile(window_2d[:,:,np.newaxis], (1, 1, n_classes))
+    weight_mask_count = np.tile(window_2d[:,:,np.newaxis], (1, 1, 1))
 
     result_queue = queue.Queue(maxsize=2)
     stop_signal = object()
@@ -150,18 +154,18 @@ def main(tile_folder: str, crop_limit=None, output_directory: str | None = None,
     mosaic_pbar = tqdm(total=total_chunks, desc="Writing", position=1)
     fetch_pbar  = tqdm(total=total_chunks, desc="Inference", position=0)
 
-    def mosaicking_worker(dst_class, dst_conf=None, dst_probs=None, dst_entropy=None, dst_gap=None, extra_data_generators=None):
+    def mosaicking_worker(dst_class, dst_conf=None, dst_probs=None, dst_entropy=None, dst_gap=None, extra_data_generators=None, weight_mask=None, weight_mask_count=None):
         try:
             while True:
                 item = result_queue.get()
                 if item is stop_signal:
                     result_queue.task_done()
                     break
-                (results, coords, H_crop, W_crop, patch_size, n_classes,
+                (results, coords, H_crop, W_crop, patch_size, n_classes_runtime,
                  c_start, r_start, W_chunk, H_chunk) = item
                 try:
                     # Accumulate probabilities
-                    avg = accumulate_probs(results, coords, H_crop, W_crop, patch_size, n_classes)
+                    avg = accumulate_probs(results, coords, H_crop, W_crop, patch_size, n_classes_runtime, weight_mask, weight_mask_count)
                     
                     # Calculate outputs
                     dominant_idx = np.argmax(avg, axis=2).astype(np.uint8)
@@ -221,7 +225,7 @@ def main(tile_folder: str, crop_limit=None, output_directory: str | None = None,
 
         worker_thread = threading.Thread(
             target=mosaicking_worker,
-            args=(dst_class, dst_conf, dst_probs, dst_entropy, dst_gap, extra_data_generators),
+            args=(dst_class, dst_conf, dst_probs, dst_entropy, dst_gap, extra_data_generators, weight_mask, weight_mask_count),
             daemon=True
         )
         worker_thread.start()
@@ -269,28 +273,6 @@ def main(tile_folder: str, crop_limit=None, output_directory: str | None = None,
         if dst_probs: dst_probs.close()
         fetch_pbar.close(); mosaic_pbar.close()
 
-    print(f"\n✅ Finished in {time.time()-t0:.2f}s")
-    print(f"Class map: {out_class_path}")
-    print(f"Max prob:  {out_conf_path}")
-    if SAVE_ENTROPY: print(f"Entropy:  {out_entropy_path}")
-    if SAVE_GAP: print(f"Gap:       {out_gap_path}")
-    if SAVE_FULL_PROBS: print(f"Full probs: {out_prob_path}")
-    if SAVE_PREVIEW_IMAGE:
-        # Need to re-open the file as it was closed in the worker thread
-        try:
-            with rasterio.open(out_class_path) as src:
-                class_mask = src.read(1)
-                # Calling the new, correctly imported function
-                save_color_mask_preview(
-                    class_mask, 
-                    out_path / "preview.png", 
-                    downscale_factor=PREVIEW_DOWNSCALE_FACTOR
-                )
-        except Exception as e:
-            print(f"❌ Error during final preview generation: {e}")
-
-    fetch_pbar.close(); mosaic_pbar.close()
-
     print(f"\n✅ Finished in {time.time()-t0:.2f}s")
     print(f"Class map: {out_class_path}")
     print(f"Max prob:  {out_conf_path}")
diff --git a/src/main.py b/src/main.py
index bc1bff3..02c29b9 100644
--- a/src/main.py
+++ b/src/main.py
@@ -6,9 +6,22 @@ import argparse
 from ben_v2.main import main
 
 if __name__ == "__main__":
-    parser = argparse.ArgumentParser(description="BigEarthNet v2.0 Analysis Pipeline")
-    parser.add_argument("--tile_folder", type=str, required=True, help="Path to the Sentinel-2 tile folder.")
-    parser.add_argument("--output_directory", type=str, required=True, help="Path to the output directory.")
+    parser = argparse.ArgumentParser(description='Run BigEarthNetv2.0 segmentation pipeline or benchmark.')
+    parser.add_argument('--tile_folder', type=str, help='Path to the Sentinel-2 tile folder.')
+    parser.add_argument('--output_folder', type=str, required=True, help='Path to the output folder.')
+    parser.add_argument('--benchmark', action='store_true', help='Run in benchmark mode.')
+    parser.add_argument('--input_dir', type=str, help='Path to the directory of tiles for benchmarking.')
+
     args = parser.parse_args()
+    print(args)
 
-    main(args.tile_folder, args.output_directory)
\ No newline at end of file
+    if args.benchmark:
+        if not args.input_dir:
+            print("❌ Error: --input_dir is required for benchmark mode.")
+        else:
+            from ben_v2.benchmark import benchmark
+            benchmark(args.input_dir, args.output_folder)
+    elif args.tile_folder:
+        main(args.tile_folder, args.output_folder)
+    else:
+        print("❌ Error: Either --tile_folder or --benchmark with --input_dir must be specified.")
